<a name="top"></a>
# üåü Awesome Unified Multimodal Models
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-green) 

*Curated papers on unifying multimodal understanding and generation from my regular reading.*

üì¨ **Have a new paper or collaboration idea?** Reach out to me at: **niuyuwei04@gmail.com**.  
ü§ù **Seeking Opportunities:** I'm eager to discuss and collaborate, especially for industry internships in unified multimodal model!

---

## üìë Table of Contents
- [2023](#2023)
- [2024](#2024)
- [2025](#2025)

---

## üìÑ Papers

### 2023
- **[2023-08-12] SEED: Planting a SEED of Vision in Large Language Model**  
  [![Static Badge](https://img.shields.io/badge/2307.08041-red?logo=arxiv)](https://arxiv.org/abs/2307.08041) [![Static Badge](https://img.shields.io/badge/SEED-black?logo=github)](https://github.com/AILab-CVC/SEED)  
  *Pioneering vision integration into LLMs.*

### 2024
- **[2024-03-22] LaVIT: Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**  
  [![Static Badge](https://img.shields.io/badge/2309.04669-red?logo=arxiv)](https://arxiv.org/abs/2309.04669) [![Static Badge](https://img.shields.io/badge/LaVIT-black?logo=github)](https://github.com/jy0205/LaVIT)  
- **[2024-04-22] SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation**  
  [![Static Badge](https://img.shields.io/badge/2404.14396-red?logo=arxiv)](https://arxiv.org/abs/2404.14396) [![Static Badge](https://img.shields.io/badge/SEED-X-black?logo=github)](https://github.com/AILab-CVC/SEED-X)
- **[2024-05-08] Emu: Generative Pretraining in Multimodality**  
  [![Static Badge](https://img.shields.io/badge/2307.05222-red?logo=arxiv)](https://arxiv.org/abs/2307.05222) [![Static Badge](https://img.shields.io/badge/Emu-black?logo=github)](https://github.com/baaivision/Emu)  
- **[2024-05-08] Emu2: Generative Multimodal Models are In-Context Learners**  
  [![Static Badge](https://img.shields.io/badge/2312.13286-red?logo=arxiv)](https://arxiv.org/abs/2312.13286) [![Static Badge](https://img.shields.io/badge/Emu2-black?logo=github)](https://github.com/baaivision/Emu2)  
- **[2024-05-16] Chameleon: Mixed-Modal Early-Fusion Foundation Models**  
  [![Static Badge](https://img.shields.io/badge/2405.09818-red?logo=arxiv)](https://arxiv.org/abs/2405.09818) [![Static Badge](https://img.shields.io/badge/Chameleon-black?logo=github)](https://github.com/facebookresearch/chameleon)  
- **[2024-08-20] Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model**  
  [![Static Badge](https://img.shields.io/badge/2408.11039-red?logo=arxiv)](https://arxiv.org/abs/2408.11039) [![Static Badge](https://img.shields.io/badge/Transfusion-black?logo=github)](https://github.com/lucidrains/transfusion-pytorch)  
- **[2024-09-27] Emu3: Next-Token Prediction is All You Need**  
  [![Static Badge](https://img.shields.io/badge/2409.18869-red?logo=arxiv)](https://arxiv.org/abs/2409.18869) [![Static Badge](https://img.shields.io/badge/Emu3-black?logo=github)](https://github.com/baaivision/Emu3)  
- **[2024-10-15] MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling**  
  [![Static Badge](https://img.shields.io/badge/2410.10798-red?logo=arxiv)](https://arxiv.org/abs/2410.10798) [![Static Badge](https://img.shields.io/badge/MMAR-black?logo=github)](https://github.com/ydcUstc/MMAR)  
- **[2024-10-17] Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2410.13848-red?logo=arxiv)](https://arxiv.org/abs/2410.13848) [![Static Badge](https://img.shields.io/badge/Janus-black?logo=github)](https://github.com/deepseek-ai/Janus)  
- **[2024-10-21] PUMA: Empowering Unified MLLM with Multi-granular Visual Generation**  
  [![Static Badge](https://img.shields.io/badge/2410.13861-red?logo=arxiv)](https://arxiv.org/abs/2410.13861) [![Static Badge](https://img.shields.io/badge/PUMA-black?logo=github)](https://github.com/rongyaofang/PUMA)  
- **[2024-10-21] Show-o: One Single Transformer to Unify Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2408.12528-red?logo=arxiv)](https://arxiv.org/abs/2408.12528) [![Static Badge](https://img.shields.io/badge/Show_o-black?logo=github)](https://github.com/showlab/Show-o)  
- **[2024-10-23] VILA-U: A Unified Foundation Model Integrating Visual Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2409.04429-red?logo=arxiv)](https://arxiv.org/abs/2409.04429) [![Static Badge](https://img.shields.io/badge/VILA_U-black?logo=github)](https://github.com/mit-han-lab/vila-u)  
- **[2024-10-31] MIO: A Foundation Model on Multimodal Tokens**  
  [![Static Badge](https://img.shields.io/badge/2409.17692-red?logo=arxiv)](https://arxiv.org/abs/2409.17692) [![Static Badge](https://img.shields.io/badge/MIO-black?logo=github)](https://github.com/MIO-Team/MIO)  
- **[2024-11-12] JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2411.07975-red?logo=arxiv)](https://arxiv.org/abs/2411.07975) [![Static Badge](https://img.shields.io/badge/JanusFlow-black?logo=github)](https://github.com/deepseek-ai/Janus)  
- **[2024-11-28] Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads**  
  [![Static Badge](https://img.shields.io/badge/2412.00127-red?logo=arxiv)](https://arxiv.org/abs/2412.00127)  
- **[2024-12-04] TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2412.03069-red?logo=arxiv)](https://arxiv.org/abs/2412.03069) [![Static Badge](https://img.shields.io/badge/TokenFlow-black?logo=github)](https://github.com/ByteFlow-AI/TokenFlow)  
- **[2024-12-05] Liquid: Language Models are Scalable Multi-modal Generators**  
  [![Static Badge](https://img.shields.io/badge/2412.04332-red?logo=arxiv)](https://arxiv.org/abs/2412.04332)  
- **[2024-12-05] MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding**  
  [![Static Badge](https://img.shields.io/badge/2411.17762-red?logo=arxiv)](https://arxiv.org/abs/2411.17762)  
- **[2024-12-08] SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation**  
  [![Static Badge](https://img.shields.io/badge/2412.05818-red?logo=arxiv)](https://arxiv.org/abs/2412.05818)  
- **[2024-12-09] ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance**  
  [![Static Badge](https://img.shields.io/badge/2412.06673-red?logo=arxiv)](https://arxiv.org/abs/2412.06673)  
- **[2024-12-09] Visual Lexicon: Rich Image Features in Language Space**  
  [![Static Badge](https://img.shields.io/badge/2412.06774-red?logo=arxiv)](https://arxiv.org/abs/2412.06774)  
- **[2024-12-11] Multimodal Latent Language Modeling with Next-Token Diffusion**  
  [![Static Badge](https://img.shields.io/badge/2412.08635-red?logo=arxiv)](https://arxiv.org/abs/2412.08635)  
- **[2024-12-12] SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding**  
  [![Static Badge](https://img.shields.io/badge/2412.09604-red?logo=arxiv)](https://arxiv.org/abs/2412.09604)  
- **[2024-12-18] MetaMorph: Multimodal Understanding and Generation via Instruction Tuning**  
  [![Static Badge](https://img.shields.io/badge/2412.14164-red?logo=arxiv)](https://arxiv.org/abs/2412.14164)
    *‚≠ê Highly recommended! The most easy training way!*  
- **[2024-12-26] LMFusion: Adapting Pretrained Language Models for Multimodal Generation**  
  [![Static Badge](https://img.shields.io/badge/2412.15188-red?logo=arxiv)](https://arxiv.org/abs/2412.15188)  
- **[2024-12-31] Dual Diffusion for Unified Image Generation and Understanding**  
  [![Static Badge](https://img.shields.io/badge/2501.00289-red?logo=arxiv)](https://arxiv.org/abs/2501.00289)  

### 2025
- **[2025-01-21] VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model**  
  [![Static Badge](https://img.shields.io/badge/2501.12327-red?logo=arxiv)](https://arxiv.org/abs/2501.12327)  
- **[2025-01-21] Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling**  
  [![Static Badge](https://img.shields.io/badge/2501.17811-red?logo=arxiv)](https://arxiv.org/abs/2501.17811)  
- **[2025-02-07] QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2502.05178-red?logo=arxiv)](https://arxiv.org/abs/2502.05178)  
- **[2025-02-17] HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2502.12148-red?logo=arxiv)](https://arxiv.org/abs/2502.12148)  
- **[2025-02-27] UniTok: A Unified Tokenizer for Visual Generation and Understanding**  
  [![Static Badge](https://img.shields.io/badge/2502.20321-red?logo=arxiv)](https://arxiv.org/abs/2502.20321)
- **[2025-03-08] USP: Unified Self-Supervised Pretraining for Image Generation and Understanding**  
  [![Static Badge](https://img.shields.io/badge/2503.06132-red?logo=arxiv)](https://arxiv.org/abs/2503.06132)
- **[2025-03-09] SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2503.06764-red?logo=arxiv)](https://arxiv.org/abs/2503.06764)  
- **[2025-03-10] WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation**  
  [![Static Badge](https://img.shields.io/badge/2503.07265-red?logo=arxiv)](https://arxiv.org/abs/2503.07265)  
  *‚≠ê Highly recommended! The only study that examines whether understanding benefits generation at the level of world knowledge. (and yes, it's my work! haha).*  
- **[2025-03-18] Unified Autoregressive Visual Generation and Understanding with Continuous Tokens**  
  [![Static Badge](https://img.shields.io/badge/2503.13436-red?logo=arxiv)](https://arxiv.org/abs/2503.13436) 
- **[2025-03-19] DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies**  
  [![Static Badge](https://img.shields.io/badge/2503.14324-red?logo=arxiv)](https://arxiv.org/abs/2503.14324)
- **[2025-03-20] Unified Multimodal Discrete Diffusion**  
  [![Static Badge](https://img.shields.io/badge/2503.20853-red?logo=arxiv)](https://arxiv.org/abs/2503.20853)  
- **[2025-03-27] UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning**  
  [![Static Badge](https://img.shields.io/badge/2503.21193-red?logo=arxiv)](https://arxiv.org/abs/2503.21193)  
- **[2025-03-27] Harmonizing Visual Representations for Unified Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2503.21979-red?logo=arxiv)](https://arxiv.org/abs/2503.21979)
- **[2025-03-27] ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement**  
  [![Static Badge](https://img.shields.io/badge/2504.01934-red?logo=arxiv)](https://arxiv.org/abs/2504.01934)
- **[2025-04-03] VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning**  
  [![Static Badge](https://img.shields.io/badge/2504.02949-red?logo=arxiv)](https://arxiv.org/abs/2504.02949)
- **[2025-04-03] UniToken: Harmonizing Multimodal Understanding and Generation through Unified Visual Encoding**  
  [![Static Badge](https://img.shields.io/badge/2504.04423-red?logo=arxiv)](https://arxiv.org/abs/2504.04423)
- **[2025-04-09] Transfer between Modalities with MetaQueries**  
  [![Static Badge](https://img.shields.io/badge/2504.06256-red?logo=arxiv)](https://arxiv.org/abs/2504.06256)  
  *‚≠ê Highly recommended!*    
- **[2025-04-20] Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens**  
  [![Static Badge](https://img.shields.io/badge/2504.14666-red?logo=arxiv)](https://arxiv.org/abs/2504.14666)  
- **[2025-04-24] Step1X-Edit: A Practical Framework for General Image Editing**  
  [![Static Badge](https://img.shields.io/badge/2504.17761-red?logo=arxiv)](https://arxiv.org/abs/2504.17761) 
- **[2025-04-29] X-Fusion: Introducing New Modality to Frozen Large Language Models**  
  [![Static Badge](https://img.shields.io/badge/2504.20996-red?logo=arxiv)](https://arxiv.org/abs/2504.20996)  
- **[2025-04-29] YoChameleon: Personalized Vision and Language Generation**  
  [![Static Badge](https://img.shields.io/badge/2504.20998-red?logo=arxiv)](https://arxiv.org/abs/2504.20998) 
- **[2025-05-01] T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT**  
  [![Static Badge](https://img.shields.io/badge/2505.00703-red?logo=arxiv)](https://arxiv.org/abs/2505.00703)
- **[2025-05-08] Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction**  
  [![Static Badge](https://img.shields.io/badge/2505.02471-red?logo=arxiv)](https://arxiv.org/abs/2505.02471) 
- **[2025-05-09] Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation**  
  [![Static Badge](https://img.shields.io/badge/2505.05472-red?logo=arxiv)](https://arxiv.org/abs/2505.05472) 
- **[2025-05-09] TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation**  
  [![Static Badge](https://img.shields.io/badge/2505.05422-red?logo=arxiv)](https://arxiv.org/abs/2505.05422)
- **[2025-05-12] Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning**  
  [![Static Badge](https://img.shields.io/badge/2505.07538-red?logo=arxiv)](https://arxiv.org/abs/2505.07538)  
- **[2025-05-15] BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset**  
  [![Static Badge](https://img.shields.io/badge/2505.09568-red?logo=arxiv)](https://arxiv.org/abs/2505.09568)
  *‚≠ê Highly recommended!*    
- **[2025-05-16] End-to-End Vision Tokenizer Tuning**  
  [![Static Badge](https://img.shields.io/badge/2505.10562-red?logo=arxiv)](https://arxiv.org/abs/2505.10562)
- **[2025-05-16] Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis**  
  [![Static Badge](https://img.shields.io/badge/2505.10046-red?logo=arxiv)](https://arxiv.org/abs/2505.10046)
- **[2025-05-21] Emerging Properties in Unified Multimodal Pretraining**  
  [![Static Badge](https://img.shields.io/badge/2505.14683-red?logo=arxiv)](https://arxiv.org/abs/2505.14683)
 *‚≠ê Highly recommended!*    
- **[2025-05-21] UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2505.14682-red?logo=arxiv)](https://arxiv.org/abs/2505.14682)
- **[2025-05-30] Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2505.23043-red?logo=arxiv)](https://arxiv.org/abs/2505.23043)
- **[2025-05-30] UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning**  
  [![Static Badge](https://img.shields.io/badge/2505.23380-red?logo=arxiv)](https://arxiv.org/abs/2505.23380)
- **[2025-05-30] Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model**  
  [![Static Badge](https://img.shields.io/badge/2505.23606-red?logo=arxiv)](https://arxiv.org/abs/2505.23606)
 *‚≠ê Highly recommended!*
- **[2025-05-30] OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2505.23661-red?logo=arxiv)](https://arxiv.org/abs/2505.23661)
- **[2025-05-30] R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation**  
  [![Static Badge](https://img.shields.io/badge/2505.23493-red?logo=arxiv)](https://arxiv.org/abs/2505.23493)
- **[2025-06-04] UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation**  
  [![Static Badge](https://img.shields.io/badge/2506.03147-red?logo=arxiv)](https://arxiv.org/abs/2506.03147) [![Static Badge](https://img.shields.io/badge/UniWorld-V1-black?logo=github)](https://github.com/PKU-YuanGroup/UniWorld-V1)
   *‚≠ê Highly recommended!*

---

## üîó Useful Links
- **[(TMLR 2025) Awesome-Autoregressive-Models-in-Vision](https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey)**  
  *You can refer this [page](https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey?tab=readme-ov-file#unified-understanding-and-generation-multi-modal-llms) for related unified models.* 
- **[Awesome-Unified-Multimodal-Models](https://github.com/showlab/Awesome-Unified-Multimodal-Models)**  
  *Another excellent resource for multimodal research.*  
- **[SOTA-Paper-Rating](https://waynejin0918.github.io/SOTA-paper-rating.io/#)**  
  *Compare and evaluate state-of-the-art papers.*

---

![Last Updated](https://img.shields.io/badge/Last%20Updated-March%2030,%202025-blue)  
*Contributions welcome! Add new papers via pull requests or email me directly.*  
[‚¨Ü Back to Top](#top)

